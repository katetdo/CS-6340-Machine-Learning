Well done! I like the functional approach you took with your code -- it is nicely written and easy to follow. Everything is implemented properly, most importantly the q-update, and your agent looks like it learned a threshold policy meaning it hits up to a point and then stands. The one item of feedback that I might give is that I'd avoid hardcoding the number of episodes the RL agent experiences (2000 in your code) because in general, different problems may have significant variation in optimal policies between runs at small episode numbers. Indeed, it looks like you had this in your example in that the threshold moved around.
